{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e926b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Continuous Unconditional Music Generation using MAESTRO Dataset\n",
    "Trains a transformer model on MIDI files and generates music continuously\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pretty_midi\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Data\n",
    "    dataset_path = \"maestro-v3.0.0\"\n",
    "    vocab_size = 128  # MIDI note range (0-127)\n",
    "    sequence_length = 512\n",
    "    \n",
    "    # Model\n",
    "    d_model = 512\n",
    "    nhead = 8\n",
    "    num_layers = 6\n",
    "    dim_feedforward = 2048\n",
    "    dropout = 0.1\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 16\n",
    "    learning_rate = 1e-4\n",
    "    epochs = 50\n",
    "    save_interval = 5\n",
    "    \n",
    "    # Generation\n",
    "    temperature = 1.0\n",
    "    max_generate_length = 1000\n",
    "    generation_interval = 10  # Generate every N seconds\n",
    "    \n",
    "    # Paths\n",
    "    model_save_path = \"music_model.pth\"\n",
    "    tokenizer_save_path = \"tokenizer.pkl\"\n",
    "    output_dir = \"generated_music\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "class MIDITokenizer:\n",
    "    \"\"\"Simple MIDI tokenizer that converts notes to tokens and back\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Special tokens\n",
    "        self.pad_token = 0\n",
    "        self.start_token = 1\n",
    "        self.end_token = 2\n",
    "        self.rest_token = 3  # For silence/rests\n",
    "        \n",
    "        # Note tokens start from 4, covering MIDI notes 0-127\n",
    "        self.note_offset = 4\n",
    "        self.vocab_size = 128 + self.note_offset\n",
    "    \n",
    "    def midi_to_sequence(self, midi_file):\n",
    "        \"\"\"Convert MIDI file to sequence of tokens\"\"\"\n",
    "        try:\n",
    "            midi = pretty_midi.PrettyMIDI(midi_file)\n",
    "            notes = []\n",
    "            \n",
    "            # Extract all notes from all instruments\n",
    "            for instrument in midi.instruments:\n",
    "                if not instrument.is_drum:  # Skip drum tracks\n",
    "                    for note in instrument.notes:\n",
    "                        notes.append((note.start, note.pitch, note.end - note.start))\n",
    "            \n",
    "            # Sort by start time\n",
    "            notes.sort(key=lambda x: x[0])\n",
    "            \n",
    "            if not notes:\n",
    "                return []\n",
    "            \n",
    "            # Convert to token sequence\n",
    "            sequence = [self.start_token]\n",
    "            current_time = 0\n",
    "            time_resolution = 0.1  # 100ms resolution\n",
    "            \n",
    "            for start_time, pitch, duration in notes:\n",
    "                # Add rest tokens for silence\n",
    "                rest_steps = int((start_time - current_time) / time_resolution)\n",
    "                sequence.extend([self.rest_token] * min(rest_steps, 10))  # Cap rests\n",
    "                \n",
    "                # Add note token\n",
    "                if 0 <= pitch <= 127:\n",
    "                    sequence.append(pitch + self.note_offset)\n",
    "                \n",
    "                current_time = start_time + duration\n",
    "            \n",
    "            sequence.append(self.end_token)\n",
    "            return sequence[:2000]  # Limit sequence length\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {midi_file}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def sequence_to_midi(self, sequence, output_file):\n",
    "        \"\"\"Convert token sequence back to MIDI file\"\"\"\n",
    "        midi = pretty_midi.PrettyMIDI()\n",
    "        instrument = pretty_midi.Instrument(program=0)  # Piano\n",
    "        \n",
    "        current_time = 0\n",
    "        time_resolution = 0.1\n",
    "        note_duration = 0.5\n",
    "        \n",
    "        for token in sequence:\n",
    "            if token == self.rest_token:\n",
    "                current_time += time_resolution\n",
    "            elif token >= self.note_offset and token < self.vocab_size:\n",
    "                pitch = token - self.note_offset\n",
    "                if 0 <= pitch <= 127:\n",
    "                    note = pretty_midi.Note(\n",
    "                        velocity=64,\n",
    "                        pitch=int(pitch),\n",
    "                        start=current_time,\n",
    "                        end=current_time + note_duration\n",
    "                    )\n",
    "                    instrument.notes.append(note)\n",
    "                current_time += time_resolution\n",
    "            elif token == self.end_token:\n",
    "                break\n",
    "        \n",
    "        midi.instruments.append(instrument)\n",
    "        midi.write(output_file)\n",
    "\n",
    "class MIDIDataset(Dataset):\n",
    "    \"\"\"Dataset for MIDI sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, seq_length):\n",
    "        self.sequences = sequences\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        if len(seq) < self.seq_length + 1:\n",
    "            # Pad sequence\n",
    "            seq = seq + [0] * (self.seq_length + 1 - len(seq))\n",
    "        else:\n",
    "            # Random crop\n",
    "            start_idx = random.randint(0, len(seq) - self.seq_length - 1)\n",
    "            seq = seq[start_idx:start_idx + self.seq_length + 1]\n",
    "        \n",
    "        return torch.tensor(seq[:-1], dtype=torch.long), torch.tensor(seq[1:], dtype=torch.long)\n",
    "\n",
    "class MusicTransformer(nn.Module):\n",
    "    \"\"\"Transformer model for music generation\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = self._generate_pos_encoding(5000, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def _generate_pos_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) * np.sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :seq_len, :].to(x.device)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class MusicGenerator:\n",
    "    \"\"\"Main class for training and generating music\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.tokenizer = MIDITokenizer()\n",
    "        self.model = None\n",
    "        self.sequences = []\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load and tokenize MIDI files from dataset\"\"\"\n",
    "        print(\"Loading MIDI dataset...\")\n",
    "        midi_files = glob.glob(os.path.join(config.dataset_path, \"**\", \"*.mid*\"), recursive=True)\n",
    "        \n",
    "        if not midi_files:\n",
    "            print(f\"No MIDI files found in {config.dataset_path}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(midi_files)} MIDI files\")\n",
    "        \n",
    "        sequences = []\n",
    "        for midi_file in tqdm(midi_files, desc=\"Processing MIDI files\"):\n",
    "            seq = self.tokenizer.midi_to_sequence(midi_file)\n",
    "            if len(seq) > 50:  # Only keep sequences with reasonable length\n",
    "                sequences.append(seq)\n",
    "        \n",
    "        self.sequences = sequences\n",
    "        print(f\"Loaded {len(sequences)} valid sequences\")\n",
    "        \n",
    "        # Save tokenizer\n",
    "        with open(config.tokenizer_save_path, 'wb') as f:\n",
    "            pickle.dump(self.tokenizer, f)\n",
    "    \n",
    "    def create_model(self):\n",
    "        \"\"\"Create and initialize the model\"\"\"\n",
    "        self.model = MusicTransformer(\n",
    "            vocab_size=self.tokenizer.vocab_size,\n",
    "            d_model=config.d_model,\n",
    "            nhead=config.nhead,\n",
    "            num_layers=config.num_layers,\n",
    "            dim_feedforward=config.dim_feedforward,\n",
    "            dropout=config.dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        print(f\"Model created with {sum(p.numel() for p in self.model.parameters())} parameters\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        if not self.sequences:\n",
    "            print(\"No sequences loaded. Please run load_dataset() first.\")\n",
    "            return\n",
    "        \n",
    "        dataset = MIDIDataset(self.sequences, config.sequence_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
    "        \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=config.learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(config.epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "            \n",
    "            for batch_idx, (input_seq, target_seq) in enumerate(progress_bar):\n",
    "                input_seq, target_seq = input_seq.to(self.device), target_seq.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(input_seq)\n",
    "                loss = criterion(output.view(-1, output.size(-1)), target_seq.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch+1} completed. Average loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Save model periodically\n",
    "            if (epoch + 1) % config.save_interval == 0:\n",
    "                self.save_model(f\"model_epoch_{epoch+1}.pth\")\n",
    "        \n",
    "        # Save final model\n",
    "        self.save_model(config.model_save_path)\n",
    "        print(\"Training completed!\")\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save model state\"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'config': config\n",
    "        }, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load model state\"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Model file {path} not found\")\n",
    "            return False\n",
    "        \n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.create_model()\n",
    "        \n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        return True\n",
    "    \n",
    "    def generate_sequence(self, length, temperature=1.0):\n",
    "        \"\"\"Generate a music sequence\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Start with start token\n",
    "        sequence = [self.tokenizer.start_token]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(length):\n",
    "                # Prepare input (last sequence_length tokens)\n",
    "                input_seq = sequence[-config.sequence_length:]\n",
    "                input_tensor = torch.tensor([input_seq], dtype=torch.long).to(self.device)\n",
    "                \n",
    "                # Get model prediction\n",
    "                output = self.model(input_tensor)\n",
    "                logits = output[0, -1, :] / temperature\n",
    "                \n",
    "                # Sample next token\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                sequence.append(next_token)\n",
    "                \n",
    "                # Stop if end token is generated\n",
    "                if next_token == self.tokenizer.end_token:\n",
    "                    break\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def continuous_generation(self):\n",
    "        \"\"\"Generate music continuously\"\"\"\n",
    "        print(\"Starting continuous music generation...\")\n",
    "        print(f\"Generating new music every {config.generation_interval} seconds\")\n",
    "        print(\"Press Ctrl+C to stop\")\n",
    "        \n",
    "        generation_count = 0\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                generation_count += 1\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                \n",
    "                print(f\"\\nGenerating piece #{generation_count} at {timestamp}\")\n",
    "                \n",
    "                # Generate sequence\n",
    "                sequence = self.generate_sequence(\n",
    "                    config.max_generate_length,\n",
    "                    config.temperature\n",
    "                )\n",
    "                \n",
    "                # Save as MIDI\n",
    "                output_file = os.path.join(\n",
    "                    config.output_dir,\n",
    "                    f\"generated_{timestamp}_{generation_count}.mid\"\n",
    "                )\n",
    "                \n",
    "                self.tokenizer.sequence_to_midi(sequence, output_file)\n",
    "                print(f\"Generated MIDI saved: {output_file}\")\n",
    "                \n",
    "                # Wait before next generation\n",
    "                time.sleep(config.generation_interval)\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\nStopped. Generated {generation_count} pieces.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8632c8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "No existing model found. Starting training...\n",
      "Loading MIDI dataset...\n",
      "Found 1276 MIDI files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MIDI files: 100%|██████████| 1276/1276 [05:44<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1276 valid sequences\n",
      "Model created with 19049604 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  42%|████▎     | 34/80 [09:03<12:14, 15.97s/it, loss=3.4456]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generator\u001b[38;5;241m.\u001b[39msequences:\n\u001b[0;32m     23\u001b[0m     generator\u001b[38;5;241m.\u001b[39mcreate_model()\n\u001b[1;32m---> 24\u001b[0m     \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Start continuous generation after training\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     generator\u001b[38;5;241m.\u001b[39mcontinuous_generation()\n",
      "Cell \u001b[1;32mIn[1], line 270\u001b[0m, in \u001b[0;36mMusicGenerator.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m input_seq, target_seq \u001b[38;5;241m=\u001b[39m input_seq\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), target_seq\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    269\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 270\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), target_seq\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    272\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 194\u001b[0m, in \u001b[0;36mMusicTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    192\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding[:, :seq_len, :]\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    193\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m--> 194\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\transformer.py:514\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    511\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 514\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    522\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\transformer.py:916\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    912\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[0;32m    913\u001b[0m         x\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m    915\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\transformer.py:941\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 941\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1426\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Main function\"\"\"\n",
    "generator = MusicGenerator()\n",
    "\n",
    "# Check if model exists\n",
    "if os.path.exists(config.model_save_path):\n",
    "    print(\"Found existing model. Loading...\")\n",
    "    if generator.load_model(config.model_save_path):\n",
    "        # Load tokenizer\n",
    "        if os.path.exists(config.tokenizer_save_path):\n",
    "            with open(config.tokenizer_save_path, 'rb') as f:\n",
    "                generator.tokenizer = pickle.load(f)\n",
    "        \n",
    "        # Start continuous generation\n",
    "        generator.continuous_generation()\n",
    "    else:\n",
    "        print(\"Failed to load model. Starting training...\")\n",
    "else:\n",
    "    print(\"No existing model found. Starting training...\")\n",
    "\n",
    "# Training pipeline\n",
    "generator.load_dataset()\n",
    "if generator.sequences:\n",
    "    generator.create_model()\n",
    "    generator.train()\n",
    "    \n",
    "    # Start continuous generation after training\n",
    "    generator.continuous_generation()\n",
    "else:\n",
    "    print(\"No training data loaded. Please check your dataset path.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
