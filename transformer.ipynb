{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f5009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import ast\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c12e624",
   "metadata": {},
   "source": [
    "Symbolic, Unconditioned Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa975f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 1276\n",
      "Example sequence length: 7263\n",
      "Vocabulary size: 1166 (tokens 4 ... 1275)\n",
      "\n",
      "Model instantiated on cpu. Total trainable parameters: 572814\n",
      "\n",
      "TransformerNextToken(\n",
      "  (token_emb): Embedding(1166, 128)\n",
      "  (pos_emb): Embedding(64, 128)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=128, out_features=1166, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "df_v2 = pd.read_csv('midi_df_v2.csv')\n",
    "\n",
    "def parse_tokens(s):\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "df_v2['token_list'] = df_v2['token_sequence'].apply(parse_tokens)\n",
    "\n",
    "all_tokens = set()\n",
    "for seq in df_v2['token_list']:\n",
    "    all_tokens.update(seq)\n",
    "all_tokens = sorted(all_tokens)\n",
    "orig2new = {orig: idx for idx, orig in enumerate(all_tokens)}\n",
    "new2orig = {idx: orig for orig, idx in orig2new.items()}\n",
    "vocab_size = len(orig2new)\n",
    "\n",
    "print(f\"Number of sequences: {len(df_v2)}\")\n",
    "print(f\"Example sequence length: {len(df_v2.loc[0, 'token_list'])}\")\n",
    "print(f\"Vocabulary size: {vocab_size} (tokens {all_tokens[0]} ... {all_tokens[-1]})\")\n",
    "\n",
    "class TransformerNextToken(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int = 128, nhead: int = 4, num_layers: int = 2, dim_ff: int = 256, dropout: float = 0.1, max_len: int = 64):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
    "            dropout=dropout, activation=\"gelu\", batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        tok_emb = self.token_emb(x) * (self.d_model ** 0.5)\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_emb(positions)\n",
    "        h = self.dropout(tok_emb + pos_emb)\n",
    "        h = self.transformer(h)\n",
    "        logits = self.fc_out(h)\n",
    "        return logits\n",
    "\n",
    "T = 64\n",
    "d_model = 128\n",
    "nhead = 4\n",
    "num_layers = 2\n",
    "dim_ff = 256\n",
    "dropout = 0.1\n",
    "\n",
    "model = TransformerNextToken(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers,\n",
    "    dim_ff=dim_ff,\n",
    "    dropout=dropout,\n",
    "    max_len=T\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel instantiated on {device}. Total trainable parameters: {total_params}\\n\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWindowDataset(Dataset):\n",
    "    def __init__(self, sequences, max_len=64):\n",
    "        self.seqs = [seq for seq in sequences if len(seq) > max_len]\n",
    "        self.total_windows = sum(len(seq) - max_len for seq in self.seqs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_windows\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = random.choice(self.seqs)\n",
    "        L = len(seq)\n",
    "        start = random.randint(0, L - 64 - 1)\n",
    "        window = seq[start : start + 64 + 1]\n",
    "        x = torch.LongTensor(window[:-1])\n",
    "        y = torch.LongTensor(window[1:])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sequences: 962,  Val sequences: 137\n",
      "Num train batches: 271595 Num val batches: 31048\n"
     ]
    }
   ],
   "source": [
    "df_v2['indexed_seq'] = df_v2['token_list'].apply(lambda seq: [orig2new[t] for t in seq])\n",
    "\n",
    "train_seqs = df_v2[df_v2['split']=='train']['indexed_seq'].tolist()\n",
    "val_seqs = df_v2[df_v2['split']=='validation']['indexed_seq'].tolist()\n",
    "\n",
    "print(f\"Train sequences: {len(train_seqs)}, Val sequences: {len(val_seqs)}\")\n",
    "\n",
    "T = 64\n",
    "train_dataset = RandomWindowDataset(train_seqs, max_len=T)\n",
    "val_dataset = RandomWindowDataset(val_seqs, max_len=T)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "print(\"Num train batches:\", len(train_loader), \"Num val batches:\", len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8979bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train] Batch 0/2000   Avg Loss: 4.6676\n",
      "Epoch 1 [Train] Batch 500/2000   Avg Loss: 4.6825\n",
      "Epoch 1 [Train] Batch 1000/2000   Avg Loss: 4.6457\n",
      "Epoch 1 [Train] Batch 1500/2000   Avg Loss: 4.6191\n",
      "Epoch 1 [Val]   Batch 0/500   Avg Loss: 4.5949\n",
      "Epoch 1 [Val]   Batch 200/500   Avg Loss: 4.4643\n",
      "Epoch 1 [Val]   Batch 400/500   Avg Loss: 4.4706\n",
      "Epoch 1  Train Loss: 4.5939   Val Loss: 4.4716\n",
      "Epoch 2 [Train] Batch 0/2000   Avg Loss: 4.4011\n",
      "Epoch 2 [Train] Batch 500/2000   Avg Loss: 4.4871\n",
      "Epoch 2 [Train] Batch 1000/2000   Avg Loss: 4.4745\n",
      "Epoch 2 [Train] Batch 1500/2000   Avg Loss: 4.4583\n",
      "Epoch 2 [Val]   Batch 0/500   Avg Loss: 4.2646\n",
      "Epoch 2 [Val]   Batch 200/500   Avg Loss: 4.3595\n",
      "Epoch 2 [Val]   Batch 400/500   Avg Loss: 4.3588\n",
      "Epoch 2  Train Loss: 4.4452   Val Loss: 4.3598\n",
      "Epoch 3 [Train] Batch 0/2000   Avg Loss: 4.4036\n",
      "Epoch 3 [Train] Batch 500/2000   Avg Loss: 4.3921\n",
      "Epoch 3 [Train] Batch 1000/2000   Avg Loss: 4.3805\n",
      "Epoch 3 [Train] Batch 1500/2000   Avg Loss: 4.3700\n",
      "Epoch 3 [Val]   Batch 0/500   Avg Loss: 4.4044\n",
      "Epoch 3 [Val]   Batch 200/500   Avg Loss: 4.2808\n",
      "Epoch 3 [Val]   Batch 400/500   Avg Loss: 4.2851\n",
      "Epoch 3  Train Loss: 4.3608   Val Loss: 4.2809\n",
      "Epoch 4 [Train] Batch 0/2000   Avg Loss: 4.4425\n",
      "Epoch 4 [Train] Batch 500/2000   Avg Loss: 4.3151\n",
      "Epoch 4 [Train] Batch 1000/2000   Avg Loss: 4.3106\n",
      "Epoch 4 [Train] Batch 1500/2000   Avg Loss: 4.3044\n",
      "Epoch 4 [Val]   Batch 0/500   Avg Loss: 4.1757\n",
      "Epoch 4 [Val]   Batch 200/500   Avg Loss: 4.1994\n",
      "Epoch 4 [Val]   Batch 400/500   Avg Loss: 4.2068\n",
      "Epoch 4  Train Loss: 4.2970   Val Loss: 4.2070\n",
      "Epoch 5 [Train] Batch 0/2000   Avg Loss: 4.2827\n",
      "Epoch 5 [Train] Batch 500/2000   Avg Loss: 4.2582\n",
      "Epoch 5 [Train] Batch 1000/2000   Avg Loss: 4.2511\n",
      "Epoch 5 [Train] Batch 1500/2000   Avg Loss: 4.2440\n",
      "Epoch 5 [Val]   Batch 0/500   Avg Loss: 4.1168\n",
      "Epoch 5 [Val]   Batch 200/500   Avg Loss: 4.1412\n",
      "Epoch 5 [Val]   Batch 400/500   Avg Loss: 4.1416\n",
      "Epoch 5  Train Loss: 4.2398   Val Loss: 4.1424\n",
      "Epoch 6 [Train] Batch 0/2000   Avg Loss: 4.1436\n",
      "Epoch 6 [Train] Batch 500/2000   Avg Loss: 4.2096\n",
      "Epoch 6 [Train] Batch 1000/2000   Avg Loss: 4.2044\n",
      "Epoch 6 [Train] Batch 1500/2000   Avg Loss: 4.1989\n",
      "Epoch 6 [Val]   Batch 0/500   Avg Loss: 4.0136\n",
      "Epoch 6 [Val]   Batch 200/500   Avg Loss: 4.0954\n",
      "Epoch 6 [Val]   Batch 400/500   Avg Loss: 4.1047\n",
      "Epoch 6  Train Loss: 4.1938   Val Loss: 4.1047\n",
      "Epoch 7 [Train] Batch 0/2000   Avg Loss: 4.1106\n",
      "Epoch 7 [Train] Batch 500/2000   Avg Loss: 4.1784\n",
      "Epoch 7 [Train] Batch 1000/2000   Avg Loss: 4.1743\n",
      "Epoch 7 [Train] Batch 1500/2000   Avg Loss: 4.1699\n",
      "Epoch 7 [Val]   Batch 0/500   Avg Loss: 4.0656\n",
      "Epoch 7 [Val]   Batch 200/500   Avg Loss: 4.0712\n",
      "Epoch 7 [Val]   Batch 400/500   Avg Loss: 4.0746\n",
      "Epoch 7  Train Loss: 4.1668   Val Loss: 4.0749\n",
      "Epoch 8 [Train] Batch 0/2000   Avg Loss: 4.0841\n",
      "Epoch 8 [Train] Batch 500/2000   Avg Loss: 4.1452\n",
      "Epoch 8 [Train] Batch 1000/2000   Avg Loss: 4.1436\n",
      "Epoch 8 [Train] Batch 1500/2000   Avg Loss: 4.1412\n",
      "Epoch 8 [Val]   Batch 0/500   Avg Loss: 3.9776\n",
      "Epoch 8 [Val]   Batch 200/500   Avg Loss: 4.0617\n",
      "Epoch 8 [Val]   Batch 400/500   Avg Loss: 4.0537\n",
      "Epoch 8  Train Loss: 4.1378   Val Loss: 4.0515\n",
      "Epoch 9 [Train] Batch 0/2000   Avg Loss: 4.0049\n",
      "Epoch 9 [Train] Batch 500/2000   Avg Loss: 4.1298\n",
      "Epoch 9 [Train] Batch 1000/2000   Avg Loss: 4.1284\n",
      "Epoch 9 [Train] Batch 1500/2000   Avg Loss: 4.1235\n",
      "Epoch 9 [Val]   Batch 0/500   Avg Loss: 4.2349\n",
      "Epoch 9 [Val]   Batch 200/500   Avg Loss: 4.0336\n",
      "Epoch 9 [Val]   Batch 400/500   Avg Loss: 4.0305\n",
      "Epoch 9  Train Loss: 4.1224   Val Loss: 4.0323\n",
      "Epoch 10 [Train] Batch 0/2000   Avg Loss: 4.1254\n",
      "Epoch 10 [Train] Batch 500/2000   Avg Loss: 4.1073\n",
      "Epoch 10 [Train] Batch 1000/2000   Avg Loss: 4.1041\n",
      "Epoch 10 [Train] Batch 1500/2000   Avg Loss: 4.1043\n",
      "Epoch 10 [Val]   Batch 0/500   Avg Loss: 3.9970\n",
      "Epoch 10 [Val]   Batch 200/500   Avg Loss: 4.0212\n",
      "Epoch 10 [Val]   Batch 400/500   Avg Loss: 4.0223\n",
      "Epoch 10  Train Loss: 4.1046   Val Loss: 4.0236\n",
      "Epoch 11 [Train] Batch 0/2000   Avg Loss: 4.0958\n",
      "Epoch 11 [Train] Batch 500/2000   Avg Loss: 4.0943\n",
      "Epoch 11 [Train] Batch 1000/2000   Avg Loss: 4.0910\n",
      "Epoch 11 [Train] Batch 1500/2000   Avg Loss: 4.0897\n",
      "Epoch 11 [Val]   Batch 0/500   Avg Loss: 4.0893\n",
      "Epoch 11 [Val]   Batch 200/500   Avg Loss: 3.9969\n",
      "Epoch 11 [Val]   Batch 400/500   Avg Loss: 4.0030\n",
      "Epoch 11  Train Loss: 4.0884   Val Loss: 4.0047\n",
      "Epoch 12 [Train] Batch 0/2000   Avg Loss: 4.2337\n",
      "Epoch 12 [Train] Batch 500/2000   Avg Loss: 4.0800\n",
      "Epoch 12 [Train] Batch 1000/2000   Avg Loss: 4.0798\n",
      "Epoch 12 [Train] Batch 1500/2000   Avg Loss: 4.0802\n",
      "Epoch 12 [Val]   Batch 0/500   Avg Loss: 3.7717\n",
      "Epoch 12 [Val]   Batch 200/500   Avg Loss: 3.9944\n",
      "Epoch 12 [Val]   Batch 400/500   Avg Loss: 3.9920\n",
      "Epoch 12  Train Loss: 4.0796   Val Loss: 3.9913\n",
      "Epoch 13 [Train] Batch 0/2000   Avg Loss: 4.0843\n",
      "Epoch 13 [Train] Batch 500/2000   Avg Loss: 4.0768\n",
      "Epoch 13 [Train] Batch 1000/2000   Avg Loss: 4.0717\n",
      "Epoch 13 [Train] Batch 1500/2000   Avg Loss: 4.0694\n",
      "Epoch 13 [Val]   Batch 0/500   Avg Loss: 4.0212\n",
      "Epoch 13 [Val]   Batch 200/500   Avg Loss: 3.9758\n",
      "Epoch 13 [Val]   Batch 400/500   Avg Loss: 3.9788\n",
      "Epoch 13  Train Loss: 4.0674   Val Loss: 3.9764\n",
      "Epoch 14 [Train] Batch 0/2000   Avg Loss: 4.0746\n",
      "Epoch 14 [Train] Batch 500/2000   Avg Loss: 4.0593\n",
      "Epoch 14 [Train] Batch 1000/2000   Avg Loss: 4.0625\n",
      "Epoch 14 [Train] Batch 1500/2000   Avg Loss: 4.0621\n",
      "Epoch 14 [Val]   Batch 0/500   Avg Loss: 3.8498\n",
      "Epoch 14 [Val]   Batch 200/500   Avg Loss: 3.9717\n",
      "Epoch 14 [Val]   Batch 400/500   Avg Loss: 3.9676\n",
      "Epoch 14  Train Loss: 4.0601   Val Loss: 3.9648\n",
      "Epoch 15 [Train] Batch 0/2000   Avg Loss: 4.0917\n",
      "Epoch 15 [Train] Batch 500/2000   Avg Loss: 4.0521\n",
      "Epoch 15 [Train] Batch 1000/2000   Avg Loss: 4.0528\n",
      "Epoch 15 [Train] Batch 1500/2000   Avg Loss: 4.0511\n",
      "Epoch 15 [Val]   Batch 0/500   Avg Loss: 4.0156\n",
      "Epoch 15 [Val]   Batch 200/500   Avg Loss: 3.9580\n",
      "Epoch 15 [Val]   Batch 400/500   Avg Loss: 3.9571\n",
      "Epoch 15  Train Loss: 4.0479   Val Loss: 3.9553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_train_batches = 2000\n",
    "max_val_batches = 500\n",
    "\n",
    "num_epochs = 15\n",
    "patience_val = 2\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        if i >= max_train_batches:\n",
    "            break\n",
    "\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), batch_y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            avg_so_far = total_train_loss / (i + 1)\n",
    "            print(f\"Epoch {epoch} [Train] Batch {i}/{max_train_batches}   Avg Loss: {avg_so_far:.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / min(len(train_loader), max_train_batches)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y) in enumerate(val_loader):\n",
    "            if i >= max_val_batches:\n",
    "                break\n",
    "\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            logits = model(batch_x)\n",
    "            loss = criterion(logits.view(-1, vocab_size), batch_y.view(-1))\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            if i % 200 == 0:\n",
    "                avg_val_so_far = total_val_loss / (i + 1)\n",
    "                print(f\"Epoch {epoch} [Val] Batch {i}/{max_val_batches} Avg Loss: {avg_val_so_far:.4f}\")\n",
    "\n",
    "    avg_val_loss = total_val_loss / min(len(val_loader), max_val_batches)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch} Train Loss: {avg_train_loss:.4f} Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_transformer_model.pt\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience_val:\n",
    "            print(f\"No improvement for {patience_val} epochs → Early stopping.\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_transformer_model.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c3148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated total tokens: 264\n",
      "Saved generated_tokens.npy (shape= 264 )\n"
     ]
    }
   ],
   "source": [
    "def generate_sequence(model, start_seq, gen_len=200, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = start_seq.copy()\n",
    "\n",
    "    for _ in range(gen_len):\n",
    "        context = generated[-T:]\n",
    "        x = torch.LongTensor(context).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "        last_logits = logits[0, -1, :]\n",
    "        probs = torch.softmax(last_logits / temperature, dim=0).cpu().numpy()\n",
    "        next_idx = np.random.choice(vocab_size, p=probs)\n",
    "        generated.append(int(next_idx))\n",
    "    return generated\n",
    "\n",
    "random_idx = random.randrange(len(val_seqs))\n",
    "seed = val_seqs[random_idx]\n",
    "if len(seed) < T:\n",
    "    seed = [seed[0]] * (T - len(seed)) + seed\n",
    "else:\n",
    "    seed = seed[:T]\n",
    "\n",
    "gen_indices = generate_sequence(model, start_seq=seed, gen_len=200, temperature=1.0)\n",
    "print(\"Generated total tokens:\", len(gen_indices))\n",
    "\n",
    "generated_original = [ new2orig[idx] for idx in gen_indices ]\n",
    "np.save(\"generated_tokens.npy\", np.array(generated_original, dtype=np.int16))\n",
    "print(\"Saved generated_tokens.npy (shape=\", len(generated_original), \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b37ae73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5cd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4223b1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5ed78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
